{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://SGN1HEC752.vn.dksh.net:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Colab</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x167fd549ad0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"Colab\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").load(r'housing.csv', header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  0|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "df = df[['id'] + df.columns[:-1]]\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "|summary|               id|          longitude|         latitude|housing_median_age|       total_rooms|    total_bedrooms|        population|       households|     median_income|median_house_value|ocean_proximity|\n",
      "+-------+-----------------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "|  count|            20640|              20640|            20640|             20640|             20640|             20433|             20640|            20640|             20640|             20640|          20640|\n",
      "|   mean|          10319.5|-119.56970445736148| 35.6318614341087|28.639486434108527|2635.7630813953488| 537.8705525375618|1425.4767441860465|499.5396802325581|3.8706710029070246|206855.81690891474|           NULL|\n",
      "| stddev|5958.399113856003|  2.003531723502584|2.135952397457101| 12.58555761211163|2181.6152515827944|421.38507007403115|  1132.46212176534|382.3297528316098| 1.899821717945263|115395.61587441359|           NULL|\n",
      "|    min|                0|            -124.35|            32.54|               1.0|               2.0|               1.0|               3.0|              1.0|            0.4999|           14999.0|      <1H OCEAN|\n",
      "|    25%|             5159|             -121.8|            33.93|              18.0|            1447.0|             296.0|             787.0|            280.0|            2.5625|          119600.0|           NULL|\n",
      "|    50%|            10318|            -118.49|            34.26|              29.0|            2127.0|             435.0|            1166.0|            409.0|            3.5347|          179700.0|           NULL|\n",
      "|    75%|            15477|            -118.01|            37.71|              37.0|            3146.0|             647.0|            1724.0|            605.0|            4.7426|          264700.0|           NULL|\n",
      "|    max|            20639|            -114.31|            41.95|              52.0|           39320.0|            6445.0|           35682.0|           6082.0|           15.0001|          500001.0|     NEAR OCEAN|\n",
      "+-------+-----------------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Processing step**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[id: bigint, longitude: double, latitude: double, housing_median_age: double, total_rooms: double, total_bedrooms: double, population: double, households: double, median_income: double, median_house_value: double, ocean_proximity: string],\n",
       " DataFrame[id: bigint, longitude: double, latitude: double, housing_median_age: double, total_rooms: double, total_bedrooms: double, population: double, households: double, median_income: double, median_house_value: double, ocean_proximity: string])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.1. Processing countinous features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features_lst = train.columns\n",
    "numerical_features_lst.remove('median_house_value')\n",
    "numerical_features_lst.remove('id')\n",
    "numerical_features_lst.remove('ocean_proximity')\n",
    "\n",
    "numerical_features_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to handle missing in the numerical features, we will use the imputation estimator for completing missing values.\n",
    "\n",
    "\n",
    "Note: In terms of missing categorical features' value, we cannot use the \"imputer\" method. Instead, using the classification model to predict the missing values or the mode value or the easiest way, remove it. Luckily, in our dataset we don't have the any missing value in categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  0|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=numerical_features_lst,\n",
    "                  outputCols=numerical_features_lst)\n",
    "\n",
    "imputer = imputer.fit(train)\n",
    "\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the Pyspark's feature, we will need to turn all these into a vector or a list of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+\n",
      "|  0|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|    [-122.23,37.88,41...|\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numerical_vector_assembler = VectorAssembler(inputCols=numerical_features_lst,\n",
    "                                             outputCol='numerical_feature_vector')\n",
    "\n",
    "train = numerical_vector_assembler.transform(train)\n",
    "test = numerical_vector_assembler.transform(test)\n",
    "\n",
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(numerical_feature_vector=DenseVector([-122.23, 37.88, 41.0, 880.0, 129.0, 322.0, 126.0, 8.3252])),\n",
       " Row(numerical_feature_vector=DenseVector([-122.22, 37.86, 21.0, 7099.0, 1106.0, 2401.0, 1138.0, 8.3014]))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here is the detail of the two vectors\n",
    "train.select('numerical_feature_vector').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with continuous values in machine learning, we need to standerdize or do some sort of pre-processing functions to those variables to get them in the same range or style. So we're gonna use do \"StandardScaler\" to all those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+\n",
      "|  0|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|    [-122.23,37.88,41...|           [-1.3285534928389...|\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|           [-1.3235592186122...|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|    [-122.24,37.85,52...|           [-1.3335477670657...|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol='numerical_feature_vector',\n",
    "                        outputCol='scaled_numerical_feature_vector',\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "scaler = scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.2. Processing catagorical features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the string values or categorical values into specific indexes, we're gonna use the StringIndex function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|ocean_category_index|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+\n",
      "|  0|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|    [-122.23,37.88,41...|           [-1.3285534928389...|                 3.0|\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|           [-1.3235592186122...|                 3.0|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|    [-122.24,37.85,52...|           [-1.3335477670657...|                 3.0|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#Unlike the numerical features, we need to keep the input column for double check if needed.\n",
    "indexer = StringIndexer(inputCol='ocean_proximity',\n",
    "                        outputCol='ocean_category_index')\n",
    "\n",
    "indexer = indexer.fit(train)\n",
    "train = indexer.transform(train)\n",
    "test = indexer.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Row(ocean_category_index=0.0),\n",
       " Row(ocean_category_index=1.0),\n",
       " Row(ocean_category_index=2.0),\n",
       " Row(ocean_category_index=3.0),\n",
       " Row(ocean_category_index=4.0)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.select('ocean_category_index').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the following step is doing the OneHotEncoder to transform the categorical values.\n",
    "\n",
    "Note: For Pyspark OneHotEncoder, it's a bit different from the normal one. Check the below link for full detail.\n",
    "https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|ocean_category_index|ocean_category_one_hot|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+\n",
      "|  0|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|    [-122.23,37.88,41...|           [-1.3285534928389...|                 3.0|         (4,[3],[1.0])|\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|           [-1.3235592186122...|                 3.0|         (4,[3],[1.0])|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|    [-122.24,37.85,52...|           [-1.3335477670657...|                 3.0|         (4,[3],[1.0])|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(inputCol='ocean_category_index',\n",
    "                                outputCol='ocean_category_one_hot')\n",
    "\n",
    "one_hot_encoder = one_hot_encoder.fit(train)\n",
    "\n",
    "train = one_hot_encoder.transform(train)\n",
    "test = one_hot_encoder.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.3. Assemble vector*\n",
    "\n",
    "For our machine learning model to understand, we have to concat the numerical vector with the OneHot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|ocean_category_index|ocean_category_one_hot|final_feature_vector|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+\n",
      "|  0|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|    [-122.23,37.88,41...|           [-1.3285534928389...|                 3.0|         (4,[3],[1.0])|[-1.3285534928389...|\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|           [-1.3235592186122...|                 3.0|         (4,[3],[1.0])|[-1.3235592186122...|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['scaled_numerical_feature_vector',\n",
    "                                       'ocean_category_one_hot'],\n",
    "                            outputCol='final_feature_vector')\n",
    "\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)\n",
    "\n",
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(final_feature_vector=DenseVector([-1.3286, 1.0557, 0.9856, -0.7967, -0.9674, -0.9581, -0.9707, 2.3394, 0.0, 0.0, 0.0, 1.0])),\n",
       " Row(final_feature_vector=DenseVector([-1.3236, 1.0463, -0.6067, 2.0284, 1.3441, 0.8415, 1.6567, 2.3269, 0.0, 0.0, 0.0, 1.0]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('final_feature_vector').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Training the model**\n",
    "\n",
    "To make it simple, we just train one LinearRegression model (Of course the decision tree, random forest model, etc is replacable in this exampl, feel free to explore it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_218f515dffbb"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='final_feature_vector',\n",
    "                      labelCol='median_house_value')\n",
    "\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_218f515dffbb, numFeatures=12"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lr.fit(train)\n",
    "\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Prediction step**\n",
    "\n",
    "After fitting the model into the training data set, we are going to the prediction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+----------------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|ocean_category_index|ocean_category_one_hot|final_feature_vector|predicted_median_house_value|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+----------------------------+\n",
      "|  0|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|    [-122.23,37.88,41...|           [-1.3285534928389...|                 3.0|         (4,[3],[1.0])|[-1.3285534928389...|           406583.1200940534|\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|           [-1.3235592186122...|                 3.0|         (4,[3],[1.0])|[-1.3235592186122...|           420948.8424817726|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|    [-122.24,37.85,52...|           [-1.3335477670657...|                 3.0|         (4,[3],[1.0])|[-1.3335477670657...|          377057.48226508644|\n",
      "|  3|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|    [-122.25,37.85,52...|           [-1.3385420412924...|                 3.0|         (4,[3],[1.0])|[-1.3385420412924...|          319721.64777797577|\n",
      "|  4|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|    [-122.25,37.85,52...|           [-1.3385420412924...|                 3.0|         (4,[3],[1.0])|[-1.3385420412924...|          254267.26805430828|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_train_df = lr.transform(train).withColumnRenamed('prediction',\n",
    "                                                      'predicted_median_house_value')\n",
    "\n",
    "pred_train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+----------------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|ocean_category_index|ocean_category_one_hot|final_feature_vector|predicted_median_house_value|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+----------------------------+\n",
      "|  7|  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|          241400.0|       NEAR BAY|    [-122.25,37.84,52...|           [-1.3385420412924...|                 3.0|         (4,[3],[1.0])|[-1.3385420412924...|          254846.13332648695|\n",
      "| 13|  -122.26|   37.84|              52.0|      696.0|         191.0|     345.0|     174.0|       2.6736|          191300.0|       NEAR BAY|    [-122.26,37.84,52...|           [-1.3435363155191...|                 3.0|         (4,[3],[1.0])|[-1.3435363155191...|           209436.4103789764|\n",
      "| 15|  -122.26|   37.85|              50.0|     1120.0|         283.0|     697.0|     264.0|        2.125|          140000.0|       NEAR BAY|    [-122.26,37.85,50...|           [-1.3435363155191...|                 3.0|         (4,[3],[1.0])|[-1.3435363155191...|          183827.40650193483|\n",
      "| 26|  -122.28|   37.85|              49.0|     1130.0|         244.0|     607.0|     239.0|       2.4597|           93800.0|       NEAR BAY|    [-122.28,37.85,49...|           [-1.3535248639726...|                 3.0|         (4,[3],[1.0])|[-1.3535248639726...|          194904.52286768242|\n",
      "| 28|  -122.28|   37.84|              50.0|     2082.0|         492.0|    1131.0|     473.0|       1.6424|          108900.0|       NEAR BAY|    [-122.28,37.84,50...|           [-1.3535248639726...|                 3.0|         (4,[3],[1.0])|[-1.3535248639726...|          175449.90942828113|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test_df = lr.transform(test).withColumnRenamed('prediction', 'predicted_median_house_value')\n",
    "\n",
    "pred_test_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the heavy-lifting in Spark have been done already, so we will convert the dataset into pandas for easy-to-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>numerical_feature_vector</th>\n",
       "      <th>scaled_numerical_feature_vector</th>\n",
       "      <th>ocean_category_index</th>\n",
       "      <th>ocean_category_one_hot</th>\n",
       "      <th>final_feature_vector</th>\n",
       "      <th>predicted_median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3104.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>3.1200</td>\n",
       "      <td>241400.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>[-122.25, 37.84, 52.0, 3104.0, 687.0, 1157.0, ...</td>\n",
       "      <td>[-1.3385420412924394, 1.0369350298890343, 1.86...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>[-1.3385420412924394, 1.0369350298890343, 1.86...</td>\n",
       "      <td>254846.133326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>2.6736</td>\n",
       "      <td>191300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>[-122.26, 37.84, 52.0, 696.0, 191.0, 345.0, 17...</td>\n",
       "      <td>[-1.3435363155191735, 1.0369350298890343, 1.86...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>[-1.3435363155191735, 1.0369350298890343, 1.86...</td>\n",
       "      <td>209436.410379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0   7    -122.25     37.84                52.0       3104.0           687.0   \n",
       "1  13    -122.26     37.84                52.0        696.0           191.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \\\n",
       "0      1157.0       647.0         3.1200            241400.0        NEAR BAY   \n",
       "1       345.0       174.0         2.6736            191300.0        NEAR BAY   \n",
       "\n",
       "                            numerical_feature_vector  \\\n",
       "0  [-122.25, 37.84, 52.0, 3104.0, 687.0, 1157.0, ...   \n",
       "1  [-122.26, 37.84, 52.0, 696.0, 191.0, 345.0, 17...   \n",
       "\n",
       "                     scaled_numerical_feature_vector  ocean_category_index  \\\n",
       "0  [-1.3385420412924394, 1.0369350298890343, 1.86...                   3.0   \n",
       "1  [-1.3435363155191735, 1.0369350298890343, 1.86...                   3.0   \n",
       "\n",
       "  ocean_category_one_hot                               final_feature_vector  \\\n",
       "0   (0.0, 0.0, 0.0, 1.0)  [-1.3385420412924394, 1.0369350298890343, 1.86...   \n",
       "1   (0.0, 0.0, 0.0, 1.0)  [-1.3435363155191735, 1.0369350298890343, 1.86...   \n",
       "\n",
       "   predicted_median_house_value  \n",
       "0                 254846.133326  \n",
       "1                 209436.410379  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_pd_df = pred_test_df.toPandas()\n",
    "\n",
    "pred_test_pd_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(predicted_median_house_value=254846.13332648695, median_house_value=241400.0),\n",
       " Row(predicted_median_house_value=209436.4103789764, median_house_value=191300.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_and_actuals = pred_test_df[['predicted_median_house_value',\n",
    "                                        'median_house_value']]\n",
    "                                    \n",
    "predictions_and_actuals_rdd = predictions_and_actuals.rdd\n",
    "\n",
    "predictions_and_actuals_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(254846.13332648695, 241400.0), (209436.4103789764, 191300.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_and_actuals_rdd = predictions_and_actuals_rdd.map(tuple)\n",
    "\n",
    "predictions_and_actuals_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Evaluate model**\n",
    "\n",
    "The final step is to evaluate predictions made on regression problems by using commonly used error metrics for regression such as:\n",
    "- Mean Squared Error\n",
    "- Root Mean Squared Error\n",
    "- Mean Absolute Error\n",
    "- R**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dung.hoang1.nguyen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Squared Error:      4766666983.009126\n",
      "Root Mean Squared Error: 69041.0528816669\n",
      "Mean Absolute Error:     50054.006186269275\n",
      "R**2:                    0.6501532687062687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "metrics = RegressionMetrics(predictions_and_actuals_rdd)\n",
    "\n",
    "s = '''\n",
    "Mean Squared Error:      {0}\n",
    "Root Mean Squared Error: {1}\n",
    "Mean Absolute Error:     {2}\n",
    "R**2:                    {3}\n",
    "'''.format(metrics.meanSquaredError,\n",
    "           metrics.rootMeanSquaredError,\n",
    "           metrics.meanAbsoluteError,\n",
    "           metrics.r2\n",
    "           )\n",
    "\n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
